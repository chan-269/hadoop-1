=================================      Installing a Single Node Hadoop Cluster   ================================================
                This document assumes that you have a ubuntu system already installed. The user
                has a sudo permission assigned. Set hostname and add it to the /etc/hosts file.


                 Create a new user.
                 sudo adduser hduser
                    -   Provide sudo permissions to the above user.
                 sudo visudo
                    - Add following line
                        hduser ALL=(ALL) NOPASSWD: ALL
                     Save the file.

                  ---      Login as the hduser   ------ .
                
                 1. Install Java
                     - The Hadoop release supports Java 11 or Java 8 only. Here we install Java 8.
                          sudo apt install openjdk-8-jdk
                     -  Confirm the Java installation using
                          java -version
                
                 2. Set variables in ~/.bashrc file.
                        #JAVA
                           export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
                           export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
                           #Hadoop Environment Variables
                           export HADOOP_HOME=/usr/local/hadoop
                           export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
                           export HADOOP_LOG_DIR=$HADOOP_HOME/logs
                           # Add Hadoop bin/ directory to PATH
                           export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
                           export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
                           export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
                           export PDSH_RCMD_TYPE=ssh
                 
                 3. Reload the .bashrc file using
                           source ~/.bashrc

                 4. Setup SSH
                           sudo apt install ssh -y
                           sudo apt install pdsh -y
                           sudo systemctl start ssh
                           sudo systemctl enable ssh

                 5. Enable passwordless SSH
                           ssh-keygen -t rsa
                                Press Enter on all prompts. This will create a .ssh folder and id_rsa and id_rsa.pub
                                file in the user home directory.
                                Copy the public key file
                           ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@localhost
                                Check using command , it should not ask for the password.
                           ssh hduser@localhost
                 
                  6. Download Hadoop
                           cd
                           wget -c -O hadoop.tar.gz https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
                           sudo mkdir /usr/local/hadoop
                           tar xvzf hadoop.tar.gz
                           sudo mv hadoop-3.2.4/* /usr/local/hadoop

                  7. Configure Hadoop
                           Create directories for Namenode and datanode.
                                 mkdir -p /usr/local/hadoop/hd_store/tmp
                                 mkdir -p /usr/local/hadoop/hd_store/namenode
                                 mkdir -p /usr/local/hadoop/hd_store/datanode
                                 sudo chown -R hduser:hduser /usr/local/hadoop
                                 sudo chmod 755 -R /usr/local/hadoop
                                 
                        cd $HADOOP_HOME/etc/hadoop
                   
                               A. Edit the hadoop-env.sh file and define following variables.
                                       #JAVA
                                       export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
                                       export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
                                       #Hadoop Environment Variables
                                       export HADOOP_HOME=/usr/local/hadoop
                                       export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
                                       export HADOOP_LOG_DIR=$HADOOP_HOME/logs
                                       export HDFS_NAMENODE_USER=hduser
                                       export HDFS_DATANODE_USER=hduser
                                       export HDFS_SECONDARYNAMENODE_USER=hduser
                                       export YARN_RESOURCEMANAGER_USER=hduser
                                       export YARN_NODEMANAGER_USER=hduser
                                       export YARN_NODEMANAGER_USER=hduser
                                       # Add Hadoop bin/ directory to PATH
                                       export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
                                       export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
                                       export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
            
                             B. Edit core-site.xml and add following.
                                       <configuration>
                                       <property>
                                       <name>hadoop.tmp.dir</name>
                                       <value>/usr/local/hadoop/hd_store/tmp</value>
                                       </property>
                                       <property>
                                       <name>fs.defaultFS</name>
                                       <value>hdfs://vlsi1:9000</value>
                                       </property>
                                       </configuration>
            
                             C. Edit yarn-site.xml and add following.
                                       <configuration>
                                       <property>
                                       <name>yarn.nodemanager.aux-services</name>
                                       <value>mapreduce_shuffle</value>
                                       </property>
                                       </configuration>
            
                            D. Edit hdfs-site.xml and add following.
                                       <configuration>
                                       <property>
                                       <name>dfs.replication</name>
                                       <value>1</value>
                                       </property>
                                       <property>
                                       <name>dfs.name.dir</name>
                                       <value>/usr/local/hadoop/hd_store/namenode</value>
                                       </property>
                                       <property>
                                       <name>dfs.data.dir</name>
                                       <value>/usr/local/hadoop/hd_store/datanode</value>
                                       </property>
                                       </configuration>
             
                           E. Edit mapred-site.xml and add following
                                       <configuration>
                                       <property>
                                       <name>mapreduce.framework.name</name>
                                       <value>yarn</value>
                                       </property>
                                       </configuration>
            
                           F. Edit workers file and add localhost entry.
                                     workers file edit
                                            nano workers
                                                 only add localhost
                                         localhost
                                            
                                    

    
                  8. Start Hadoop daemons
                         cd $HADOOP_HOME/sbin
                         hadoop namenode -format
                         start-dfs.sh
                         Check using jps command
                         Start-yarn.sh
                        Check using jps command.
                        If you get all daemons running, it means your single node Hadoop cluster is ready.



                



============================================================================================================================================



       



       touch test{1..20}.txt

       - create a dir by name data in hdfs copy all above txt files in data dir on hdfs
                touch test{1..20}.txt                                     ............................ making the text file 1 to 20
                hdfs dfs -mkdir /data                                     ............................ making the dir data in hdfs
                hdfs dfs -ls /                                            ............................ listing the dir and files by -ls in hdfs
                hdfs dfs -put test{1..20}.txt /data                       ............................ puting all 1 to 20 files in data dir
                hdfs dfs -ls /data                                        ............................ listing the 1 to 20 files in data dir is put




        -  to view the content inside the file use the below command
                 hdfs dfs -cat /demo.txt

       -   to copy from hdfs(hadoop)  to the local machine(ubuntu) use the below command
                  hdfs dfs -copyToLocal /data/*.txt .                            ..................... it will copy from hdfs and paste in data dir

       -   to copy from local machine(ubuntu) and paste in hdfs(hadoop)
                  hdfs dfs -copyFromLocal dst_path(in hdfs at what location you want to paste put that path here)

      -     to remove dir use below command
                 hdfs dfs -rm -r -f /data
                 rm -rf *.txt                ...........   all txt extension file will be deleted.
                 
                
      -     combine(merge) the two output file in one file   use the below command
                 hdfs dfs -getmerge /output1/test1.txt /output2/test2.txt  output.txt .            
                                      in output dir the test1.txt and test2.txt file there that will get merge and output will store in output.txt 
                                      file in current dir(last dot)
                                      
            






                     hdfs to local
                           -ls /
                           mkdir /acts
                           copyFromLocal src dst
                           copyToLocal   src dst
                           put
                           get
                           moveFromLocal


                    hdfs-file to hdfs-file
                           -rm -r -f dir_name
                           -mv move or rename hdfs_file/dir
                           -cp copy
                           -cat


     Q]   create a hadoop single node cluster
          create 2 files by name file1.txt and file2.txt on linux machine. create a dir by name data in hdfs. move both the files in the data 
          dir on hdfs . create a another dir by name reports in hdfs . copy file2.txt from data dir on hdfs and put on reports dir. then rename 
          this file to old.txt . Then display the contents of the old.txt file .


             touch file1.txt
             touch file2.txt
             hdfs dfs -mkdir /data
             hdfs dfs -copyFromLocal file1.txt/data
             hdfs dfs -copyFromLocal file2.txt/data
             hdfs dfs -mkdir /reports
             hdfs dfs -put data/file2.txt reports/file2.txt
             hdfs dfs -mv file2.txt old.txt
             hdfs dfs -cat old.txt





==============================================================================================================================


              wordcount  in single node
         

on master

google it
   - apache hadoop wordcount program
   - then click on first link  (apache hadoop map reduce) and then click on wordcount v1.0 then the word count program will open.
   - copy the code

come to terminal
    make a dir
    nano WordCount.java
         paste the code in this file which is copied
     below is the code of wordcount in java hadoop


import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}

   -----------   the file name should be same as class name


 echo $(classpath)                                    ...  it will print the path
 export HADOOP_CLASSPATH=$(hadoop classpath)          ...  path will be store
 javac -classpath $HADOOP_CLASSPATH WordCount .java   ...  it will compile it
 ls
 jar cf wc.jar WordCount*.class                       ...  it will create a jar file of all class
 nano words.txt
    write into this file
       hii
       hello world
       bbye
 jps                                    ....  start the cluster if not
 hdfs dfs -mkdir /wordcount
 hdfs dfs -mkdir /wordcount/input       .... created input dir in wordcount dir
 hdfs dfs -put words.txt /wordcount/input  .... put the words.txt file in wordcount/input here
 hadoop jar wc.jar WordCount /wordcount/input /wordcount/output      .... run command
 start-all.sh

 hdfs dfs -ls /wordcount
 hdfs dfs -ls /wordcount/output
 hdfs dfss -cat /wordcount/output/part-r-00000

-----------------------------------------------------------------------------------
for single cluster (if error comes) in below file
        nano mapred-site.xml
           HADOOP_HOME

for multiple-cluster
         nano mapred-site.xml
            <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
         scp mapred-site.xml DN1:/usr/local/hadoop/etc/hadoop
         scp mapred-site.xml DN2:/usr/local/hadoop/etc/hadoop
         scp mapred-site.xml DN3:/usr/local/hadoop/etc/hadoop

      start-all.sh
      hadoop jar wc.jar WordCount /wordcount/input /wordcount/output  

